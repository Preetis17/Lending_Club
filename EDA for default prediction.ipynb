{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"verdana\"> \n",
    "\n",
    "# Exploratory Data Analysis for default prediction for Lending Club dataset\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "In this section we are going to:\n",
    "\n",
    "<br>Understand dataset\n",
    "<br>clean/impute missing values\n",
    "<br>Dive into each attribute\n",
    "<br>Record observations and create a clean data for model building\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:/Users/Preeti/Github/Lending_Club/plots/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter('ignore',DeprecationWarning)\n",
    "import seaborn as sns\n",
    "import time\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from time import strftime\n",
    "\n",
    "\n",
    "from pylab import rcParams\n",
    "#import hdbscan\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import confusion_matrix as conf\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('loan.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"verdana\"> \n",
    "  \n",
    "  ### Understanding dataset\n",
    "  <br>\n",
    "  We will look at sample records, number of records, number of columns, if there are any duplicate records, identify categorical variables, identify numeric variables, range of values for numeric variables\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "tabulate(print(\"\\n1. Total number of records: \",df.shape[0],\n",
    "               \"\\n2. Total number of columns: \",df.shape[1],\n",
    "               \"\\n3. Column Names : \",df.columns,\n",
    "               \"\\n4. Are there any duplicate records?\", df.duplicated().any())\n",
    "                         , tablefmt='textile')\n",
    "print(\"\\n******* Numerical attributes and their range *******\")\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font face=\"verdana\"> \n",
    "Let us look at sample of each column in the dataset, Look at missing values in each columns and make our observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = df.columns\n",
    "for col in col_names:\n",
    "    print(\"Column Name: \",col)\n",
    "    df[col].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].isnull().sum() > 0.0:\n",
    "        print(\"#Missing values in \",col,\": \",df[col].isnull().sum(),\",  % missing :\" ,round((df[col].isnull().sum()/df.shape[0])*100, 4))\n",
    "        #print(\"% of missing value in \",col,\" \",round((df[col].isnull().sum()/df.shape[0])*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"verdana\"> \n",
    "\n",
    "###  Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(15, 13))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "#save plots for later use\n",
    "plot_type = \"Correlation_\"\n",
    "plot_name = \"Matrix\"\n",
    "data_file_ext = \".png\"\n",
    "plt_file_2_save = data_dir + plot_type + plot_name + data_file_ext\n",
    "plt.savefig(plt_file_2_save)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr[corr['loan_amnt'] < 0]['loan_amnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr[corr['total_pymnt'] > 0.6]['total_pymnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr[corr['recoveries'] > 0.6]['recoveries']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"verdana\"> \n",
    "  \n",
    "  ### Observations and next steps:\n",
    "\n",
    "<br>1. Column <b>Term</b> is categorical because of the usuage \"month\". It would be good to convert it to numeric as values 36, 60 have different weightage.\n",
    "<br>2. <b>emp_length</b> impute 'n/a' to '0 year'. \n",
    "<br>3. <b>revol_util</b> has %. This needs to be removed and column has to be converted to numeric.\n",
    "<br>4. <b>issue_d</b> does not provide year of issue. This column would be incorrect to use. We can drop the column.\n",
    "<br>5. <b>earliest_cr_line</b> has year value, that would be useful in model building. We can remove the month and keep the year by cleaning it up to 4 numbers.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Missing values and imputing:\n",
    "<br>1. <b>mths_since_last_delinq</b> has missing values. These are never delinquent records, we can mark NANs to 0.\n",
    "<br>2. <b>mths_since_last_record</b> has too many missing values. We will mark NANs to 0 and observe the data.\n",
    "<br>3. <b>dti</b> is missing 2 values. We can impute mean values.\n",
    "<br>4. <b>delinq_2yrs,pub_rec_bankruptcies,pub_rec,inq_last_6mths,collections_12_mths_ex_med,tot_coll_amt</b> We will mark NANs to 0. For collections we are going with the assumtion that there no amount to be collected and imputing mean value might cause inconsistency with policies with low loan amount.\n",
    "<br>open_acc and total_acc, The number of open credit lines in the borrower's credit file. These are missing #s but we can impute these values to 1, as borrower has to have atleast one credit line to get the loan.  We will mark NANs to 1.\n",
    "<br>10.<b>revol_util</b> these are missing values. We will impute mean values.\n",
    "<br>11.<b>annual_inc</b> these are missing values. We will impute mean values.\n",
    "<br>   <b>emp_title</b> would be insteresting to anlyze, we will impute missing values to a new category 'Others'\n",
    "\n",
    "highly correlated columns\n",
    "<br>1.<b>funded_amnt,funded_amnt_inv,installment,out_prncp,out_prncp_inv</b> are higly correlated with loan amount, we will drop this column.\n",
    "<br>2. <b>total_pymnt_inv,total_rec_prncp,total_rec_int,last_pymnt_amnt</b> are higly correlated with total_pymnt, we will drop this column.\n",
    "<br>2. <b>collection_recovery_fee</b> is higly correlated with recoveries, we will drop this column.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br> The following are missing more than 80% of the data, Hence we are dropping these columns:\n",
    "<br>-mths_since_last_record\n",
    "<br>-open_il_6m\n",
    "<br>-open_il_12m\n",
    "<br>-open_il_24m\n",
    "<br>-mths_since_rcnt_il\n",
    "<br>-total_bal_il \n",
    "<br>-il_util \n",
    "<br>-open_rv_12m \n",
    "<br>-open_rv_24m\n",
    "<br>-max_bal_bc\n",
    "<br>-all_util\n",
    "<br>-annual_inc_joint\n",
    "<br>-dti_joint\n",
    "<br>-inq_fi\n",
    "<br>-total_cu_tl\n",
    "<br>-inq_last_12m\n",
    "<br>-verification_status_joint\n",
    "<br>-mths_since_last_major_derog\n",
    "\n",
    "<br>Note: All numeric values after imputing will have to recasted to type numeric.\n",
    "\n",
    "The above observations are going to be executed step-by-step to complete data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_high_correlation = ['collection_recovery_fee','total_pymnt_inv','total_rec_prncp','total_rec_int','last_pymnt_amnt','out_prncp_inv','out_prncp','funded_amnt','funded_amnt_inv','installment']\n",
    "for col in cols_with_high_correlation:\n",
    "    del df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 fill emp_title with others\n",
    "df['emp_title'].fillna('Others',inplace=True)\n",
    "df['emp_title'].unique()\n",
    "df['emp_title'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.  Loan term has been converted to numeric and stored in the different attribute n_term\n",
    "df['term'] = df['term'].str.strip()\n",
    "df['term'].unique()\n",
    "df['n_term'] = np.where(df['term']=='36 months', 36, 60)\n",
    "df['n_term'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. <b>emp_length</b> impute 'n/a' to '0 year'. \n",
    "df['emp_length'].replace('n/a','0 years',inplace=True)\n",
    "df['emp_length'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.  fill up missing values for mths_since_last_delinq with 0\n",
    "df['mths_since_last_delinq'].fillna(0, inplace=True)\n",
    "df['mths_since_last_delinq'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. <b>mths_since_last_record</b> has too many missing values. We will mark NANs to 0 and observe the data.\n",
    "df['mths_since_last_record'].fillna(0, inplace=True)\n",
    "df['mths_since_last_record'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.revol_util has %. This needs to be removed and column has to be converted to numeric.\n",
    "#10.revol_util these are missing values. We will impute mean values.\n",
    "#df['revol_util'] = df['revol_util'].str.replace('%','')\n",
    "# fill missing value with mean values\n",
    "df['revol_util'].fillna(value=df['revol_util'].mean(),inplace=True)\n",
    "df['revol_util'] = df['revol_util'].astype(float)\n",
    "df['revol_util'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<br>8. <b>dti</b> is missing 2 values. We can impute mean values.\n",
    "df['dti'].fillna(value=df['dti'].mean(),inplace=True)\n",
    "df['dti'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. <b>earliest_cr_line</b> moves inconsistently form mm-yy to dd-mm values. We will look at more records and decide to delete.\n",
    "df['earliest_cr_line_year'] = df.earliest_cr_line.str.split('-').str[1]\n",
    "df['earliest_cr_line_year'].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['earliest_cr_line_year'] = df['earliest_cr_line_year'].astype(float)\n",
    "df['earliest_cr_line_year'].fillna(value=df['earliest_cr_line_year'].mean(),inplace=True)\n",
    "df['earliest_cr_line_year'].isnull().sum()\n",
    "df['earliest_cr_line_year'] = df['earliest_cr_line_year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['annual_inc'].fillna(value=df['annual_inc'].mean(),inplace=True)\n",
    "df['annual_inc'] = df['annual_inc'].astype(float)\n",
    "df['annual_inc'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. <b>delinq_2yrs,pub_rec_bankruptcies,open_acc,pub_rec,total_acc</b> We will mark NANs to 0.\n",
    "cols_NAN_to_0=['delinq_2yrs','inq_last_6mths','open_acc','pub_rec','total_acc','collections_12_mths_ex_med','tot_coll_amt' ]\n",
    "for col in cols_NAN_to_0:\n",
    "    df[col].fillna(0,inplace=True)\n",
    "    df[col] = df[col].astype(float)\n",
    "    print(\"Is \",col,\" missing values?\",df[col].isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delet columns with more than 80% missing values\n",
    "del df['issue_d']\n",
    "del df['earliest_cr_line']\n",
    "del df['mths_since_last_record']\n",
    "del df['open_il_6m']\n",
    "del df['open_il_12m']\n",
    "del df['open_il_24m']\n",
    "del df['mths_since_rcnt_il']\n",
    "del df['total_bal_il']\n",
    "del df['il_util']\n",
    "del df['open_rv_12m'] \n",
    "del df['open_rv_24m']\n",
    "del df['max_bal_bc']\n",
    "del df['all_util']\n",
    "del df['annual_inc_joint']\n",
    "del df['dti_joint']\n",
    "del df['inq_fi']\n",
    "del df['total_cu_tl']\n",
    "del df['inq_last_12m']\n",
    "del df['verification_status_joint']\n",
    "del df['mths_since_last_major_derog']\n",
    "del df['acc_now_delinq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['acc_now_delinq']\n",
    "del df['tot_cur_bal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].isnull().sum() > 0.0:\n",
    "        print(\"#Missing values in \",col,\": \",df[col].isnull().sum(),\",  % missing :\" ,round((df[col].isnull().sum()/df.shape[0])*100, 4))\n",
    "        #print(\"% of missing value in \",col,\" \",round((df[col].isnull().sum()/df.shape[0])*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['emp_title','purpose']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['open_acc_6m']\n",
    "del df['title']\n",
    "del df['last_credit_pull_d']\n",
    "del df['total_rev_hi_lim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df.select_dtypes(['number'])\n",
    "numeric_col_names = df_numeric.columns.values.tolist()\n",
    "numeric_col_names.remove('id')\n",
    "numeric_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numeric_col_names:\n",
    "    print(col)\n",
    "    plt.hist(df[col], bins = 10)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    #save plots for later use\n",
    "    plot_type = \"Frequency_\"\n",
    "    plot_name = str(col)\n",
    "    data_file_ext = \".png\"\n",
    "    plt_file_2_save = data_dir + plot_type + plot_name + data_file_ext\n",
    "    plt.savefig(plt_file_2_save)\n",
    "    \n",
    "    #plt.title(\"Frequency distribution of \",col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"verdana\"> \n",
    "    \n",
    "  \n",
    "  ### Observations and next steps:\n",
    "  \n",
    "  All of the attributes are non-linear. We should consider log transformation or higer order terms.\n",
    "  \n",
    "  # TODO : Log transform all numeric fields and replot KDE\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "For each of the categorical variables, let us identify unique values\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Revist Correlation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(15, 13))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "#save plots for later use\n",
    "plot_type = \"final_dataset_Correlation_\"\n",
    "plot_name = \"Matrix\"\n",
    "data_file_ext = \".png\"\n",
    "plt_file_2_save = data_dir + plot_type + plot_name + data_file_ext\n",
    "plt.savefig(plt_file_2_save)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"verdana\"> \n",
    "    <br>\n",
    "    <br>\n",
    "    Cells in red are highly correlated columns. We can drop columns which are highly correlated with loan_amt, like number of terms, revol_bal, annual_inc.\n",
    "    \n",
    "    \n",
    "### Scatter plots for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.scatter_matrix(df, figsize=(23, 23))\n",
    "#save plots for later use\n",
    "plot_type = \"scatter_\"\n",
    "plot_name = \"Matrix\"\n",
    "data_file_ext = \".png\"\n",
    "plt_file_2_save = data_dir + plot_type + plot_name + data_file_ext\n",
    "plt.savefig(plt_file_2_save)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"verdana\"> \n",
    "    \n",
    "  ## TODO observations on correlation map and scatter plots.\n",
    "    \n",
    "### correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('clean_data_for_cluster.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"verdana\"> \n",
    "    \n",
    "### Analysis of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "#categorical_cols = categorical_cols.drop(['issue_d','earliest_cr_line','revol_util'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"verdana\"> \n",
    "    \n",
    "#### The following are the categorical variables in our dataset. For each of these categories, we will plot number records by its unique category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    print(\"Unique values for the column \",col)\n",
    "    df[col].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"verdana\"> \n",
    "  \n",
    "  ### Observations and next steps:\n",
    "<br>Categorical columns look clean. We will proceed to plot each categorical variables.\n",
    "\n",
    "For each categorical variable, sliced by output variable <b>loan status</b>, we will plot count of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    if col == 'addr_state' or col == 'purpose':\n",
    "        sns.set(rc={'figure.figsize':(15,10.2)})\n",
    "    else:\n",
    "        sns.set(rc={'figure.figsize':(7,5)})\n",
    "    sns.countplot(y=col,data=df,palette='Set1',hue=\"loan_status\")\n",
    "    \n",
    "    #save plots for later use\n",
    "    plot_type = \"Count_of_\"\n",
    "    plot_name = str(col)\n",
    "    data_file_ext = \".png\"\n",
    "    plt_file_2_save = data_dir + plot_type + plot_name + data_file_ext\n",
    "    plt.savefig(plt_file_2_save)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"verdana\"> \n",
    "    \n",
    "#### Observations: \n",
    "<br> 1. Data for charged off is much less than fully paid.\n",
    "<br> 2. Most purpose of the loan is for debt consolidation or credit card, while wedding, renewable energy and education are least reasons to get take loan.\n",
    "<br> 3. Number of loans in California is hightest, almost twice the next highest Florida.\n",
    "<br> 4. There are fewer home owners to take a loan than those on Rent or having home loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"verdana\"> \n",
    "\n",
    "### Loan amount - In-depth analysis\n",
    "\n",
    "Let us look at how loan amount is impacting other variables\n",
    "\n",
    "#### Understanding violin plots:\n",
    "1. At the stomach of the violin is the box plot that gives median value for that category.\n",
    "2. The spread is the kernal density distribution, number of data values in that range. a peak can be understood as high concentration of data and narrow representa low number of data. \n",
    "3. overall violin gives the range of data in that category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "for col in categorical_cols:\n",
    "    if col != 'addr_state': \n",
    "        if col == 'purpose'  or col == 'emp_length':\n",
    "            sns.set(rc={'figure.figsize':(19,10)})\n",
    "        else:\n",
    "            sns.set(rc={'figure.figsize':(7,4)})\n",
    "        sns.violinplot(x=col, y=\"loan_amnt\", data=df,palette='rainbow',hue=\"loan_status\")\n",
    "        \n",
    "        #save plots for later use\n",
    "        plot_type = \"Violin_\"\n",
    "        plot_name = str(col)\n",
    "        data_file_ext = \".png\"\n",
    "        plt_file_2_save = data_dir + plot_type + plot_name + data_file_ext\n",
    "        plt.savefig(plt_file_2_save)\n",
    "    \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"verdana\"> \n",
    "    \n",
    "#### Observations: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_state = df[['loan_amnt', 'addr_state']]\n",
    "loan_by_state = df_state.groupby(['addr_state']).sum()\n",
    "loan_by_state.reset_index(inplace=True)\n",
    "loan_by_state['loan_amnt'] = round((loan_by_state['loan_amnt'].astype(float))/1000000,0)\n",
    "loan_by_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go \n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(type='choropleth',\n",
    "            colorscale = 'Portland',\n",
    "            locations = loan_by_state['addr_state'], \n",
    "            #df_addr_state['addr_state'],\n",
    "            z = loan_by_state['loan_amnt'],\n",
    "            locationmode = 'USA-states',\n",
    "            #text = loan_by_state['addr_state'],\n",
    "            marker = dict(line = dict(color = 'rgb(255,255,255)',width = 2)),\n",
    "            colorbar = {'title':\"Million USD\"}\n",
    "            ) \n",
    "layout = dict(title = 'Distribution of Loan Amount by State',\n",
    "              geo = dict(scope='usa',\n",
    "                         showlakes = True,\n",
    "                         lakecolor = 'rgb(85,173,240)')\n",
    "             )\n",
    "choromap = go.Figure(data = [data],layout = layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_notebook_mode(connected=True) \n",
    "iplot(choromap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,11)})\n",
    "sns.barplot(x=\"loan_amnt\", y=\"addr_state\", data=loan_by_state,palette=\"Set1\")\n",
    "plt.title(\"Distribution of Loan Amount by State\")\n",
    "#save plots for later use\n",
    "plot_type = \"Loan_amount_by_state_bar\"\n",
    "plot_name = str(col)\n",
    "data_file_ext = \".png\"\n",
    "plt_file_2_save = data_dir + plot_type + plot_name + data_file_ext\n",
    "plt.savefig(plt_file_2_save)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    if col == 'addr_state' or col == 'purpose'  or col == 'emp_length':\n",
    "        sns.set(rc={'figure.figsize':(15,8.27)})\n",
    "    else:\n",
    "        sns.set(rc={'figure.figsize':(5,4)})\n",
    "    sns.swarmplot(x=col, y=\"loan_amnt\", data=df,hue='loan_status',palette='Set1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
